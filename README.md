# ğŸ¥ MedGuard AI

**Multi-Agent Healthcare Data Validator with Privacy-Preserving Intelligence**

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Hackathon: Convergence 2026](https://img.shields.io/badge/Hackathon-Convergence%202026-green.svg)](https://github.com/Varsh1009/meddata-guardian)

---

## ğŸ“‹ Overview

MedGuard AI is an intelligent healthcare data quality and fairness auditor that detects data quality issues and demographic bias **before** AI models are trained, preventing biased algorithms from harming patients.

**The Problem:** Medical AI systems often exhibit significant performance disparities across demographic groups, discovered only after deployment when patients are already harmed. Example: Apple Watch ECG algorithm was 34% less accurate for Black patients after 2 years of deployment.

**Our Solution:** Multi-agent AI system that analyzes datasets in 30 seconds, providing exact, actionable recommendations with HIPAA-compliant privacy-preserving architecture.

---

## ğŸ¯ Key Features

### **1. Privacy-Preserving Synthetic Data Layer**
- Generates statistical twins of real datasets
- AI agents analyze synthetic data only (HIPAA-safe)
- Zero exposure of Protected Health Information
- Industry-standard synthetic data generation

### **2. Multi-Agent AI System**
- **Medical Advisor Agent:** Clinical context from medical literature (RAG)
- **Fairness Specialist Agent:** Exact bias quantification and mitigation strategies
- **Deployment Strategist Agent:** Week-by-week implementation roadmaps

### **3. Exact Quantification (Not Vague Advice)**
- "Recruit 552 female patients" (not "get more data")
- "11 out of 100 women misdiagnosed" (not "some bias")
- "$60,000 recruitment cost, 6-month timeline" (not "expensive, takes time")

### **4. Three-Layer HIPAA Compliance**
- **Layer 1:** Pre-flight PHI scanner (rejects before loading)
- **Layer 2:** Synthetic data firewall (AI never sees real patients)
- **Layer 3:** Local processing (no data transmission)

### **5. Context-Aware Recommendations**
- Adapts to user's timeline, budget, model type, use case
- Multiple valid paths with trade-off analysis
- Immediate fixes + long-term strategies

---

## ğŸš€ Quick Start

### **Installation**
```bash
# Clone repository
git clone https://github.com/Varsh1009/meddata-guardian.git
cd meddata-guardian

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Ollama (for AI agents)
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2:3b
```

### **Run the Application**
```bash
# Activate virtual environment
source venv/bin/activate

# Run Streamlit dashboard
streamlit run src/ui/app_complete.py
```

Open browser at `http://localhost:8501`

---

## ğŸ“Š Demo Datasets

Four synthetic datasets are included for demonstration:

1. **dataset1_heart_disease_quality.csv** - Data quality issues (missing values, duplicates, outliers)
2. **dataset2_diabetes_gender_bias.csv** - Gender bias (73% male, 27% female)
3. **dataset3_heart_disease_indigenous.csv** - Race bias (0% Indigenous representation)
4. **dataset4_diabetes_combined.csv** - Combined quality + bias issues

---

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Upload â†’ PHI Scan â†’ Synthetic Twin   â”‚
â”‚                    â†“                        â”‚
â”‚         Multi-Agent AI Analysis             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Medical  â”‚  â”‚ Fairness â”‚  â”‚Deploymentâ”‚ â”‚
â”‚  â”‚ Advisor  â”‚  â”‚Specialistâ”‚  â”‚Strategistâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                    â†“                        â”‚
â”‚   Exact Recommendations + Working Code     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technology Stack

**Core:**
- Python 3.11
- Pandas, NumPy, SciPy
- Scikit-learn, Fairlearn, imbalanced-learn

**AI/ML:**
- Ollama (local LLM runtime)
- Llama 3.2 (3B parameters)
- Instructor (schema enforcement, anti-hallucination)
- Pydantic (data validation)

**Synthetic Data:**
- Custom generator (statistical twin creation)
- Privacy validation metrics

**RAG (Retrieval-Augmented Generation):**
- ChromaDB (local vector database)
- Medical knowledge base (clinical guidelines, FDA requirements)

**UI:**
- Streamlit (web dashboard)
- Plotly (interactive visualizations)

---

## ğŸ“‚ Project Structure
```
meddata-guardian/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ synthetic/              # Demo datasets
â”œâ”€â”€ knowledge_base/
â”‚   â”œâ”€â”€ medical/                # Clinical guidelines for RAG
â”‚   â”œâ”€â”€ regulatory/             # FDA requirements
â”‚   â””â”€â”€ statistical/            # Statistical best practices
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                 # AI agents
â”‚   â”‚   â”œâ”€â”€ medical_advisor_v2.py
â”‚   â”‚   â”œâ”€â”€ fairness_specialist.py
â”‚   â”‚   â””â”€â”€ deployment_strategist.py
â”‚   â”œâ”€â”€ utils/                  # Core utilities
â”‚   â”‚   â”œâ”€â”€ phi_scanner.py
â”‚   â”‚   â”œâ”€â”€ data_quality.py
â”‚   â”‚   â”œâ”€â”€ bias_detection.py
â”‚   â”‚   â”œâ”€â”€ synthetic_generator.py
â”‚   â”‚   â””â”€â”€ medical_rag.py
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ app_complete.py     # Main Streamlit app
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ generate_datasets.py   # Synthetic data creation
â””â”€â”€ requirements.txt
```

---

## ğŸ”¬ Problem Statements Addressed

### **Problem 1: Data Quality Issues Discovered Too Late**
Healthcare datasets contain missing values, duplicates, outliers that are often found after months of model development, wasting research time and resources.

**Our Solution:** Instant detection with verified Python code to fix issues immediately.

### **Problem 2: Hidden Bias Leads to Unfair AI**
Demographic bias (gender, race, age, socioeconomic) in training data causes AI models to perform poorly for underrepresented groups, perpetuating healthcare disparities.

**Our Solution:** Detects ALL bias types (not just gender/race), quantifies exact harm, provides specific recruitment strategies.

---

## ğŸ’¡ Innovation Highlights

### **1. Synthetic Data Privacy Firewall** (Unique Approach)
- AI never sees real patient data
- Statistical twins preserve properties without PHI exposure
- Enables AI analysis with zero HIPAA risk

### **2. Anti-Hallucination Architecture**
- Instructor schemas enforce strict output format
- RAG grounds AI in real medical literature
- Python calculates exact numbers (AI can't make up statistics)
- Fallback to rule-based if AI fails

### **3. Context-Aware Intelligence**
- Asks about user's timeline, budget, model, use case
- Recommendations adapt to constraints
- "60-day timeline too short for recruitment â†’ use SMOTE + plan 6-month retrain"

### **4. Exact Quantification**
- Not "you have bias" â†’ "11 out of 100 women will be misdiagnosed"
- Not "collect data" â†’ "Recruit 552 female patients from Brigham Women's Hospital, $60K, 6 months"
- Not "use SMOTE" â†’ Shows exact Python code with expected accuracy improvement

---

## ğŸ“Š Use Cases

**Research:** Validate dataset before starting study, ensure representative enrollment  
**Clinical Deployment:** Pre-deployment fairness audit, prevent biased algorithms  
**FDA Submission:** Demographic subgroup analysis, compliance documentation  
**Healthcare Equity:** Ensure Indigenous, minority populations represented  

---

## ğŸ”’ HIPAA Compliance

MedGuard AI is designed with healthcare privacy as a core principle:

1. **Pre-Processing PHI Scan:** Validates de-identification before any data loading
2. **Synthetic Data Firewall:** AI analyzes statistical twins, not real patients
3. **Local Processing:** All computation on user's machine, no external transmission
4. **No Data Storage:** Data purged from memory when application closes
5. **Audit Trail:** Compliance documentation for regulatory review

**Deployment Model:** Desktop application, not cloud service. User maintains full data custody.

---

## ğŸ† Team: The Guardians

**Convergence 2026 - GSG x GWiSE Interdisciplinary Graduate Hackathon**

**Team Members:**
- Computer Science: AI/ML implementation, synthetic data generation
- Life Sciences: Clinical validation, medical knowledge curation
- Regulatory Affairs: HIPAA compliance, FDA requirements
- Business Strategy: Market analysis, deployment planning
- Ethics: Health equity focus, vulnerable population protection

---

## ğŸ“ˆ Impact

**Time Saved:** 30-second analysis vs. 6 months of post-hoc bias discovery  
**Patient Safety:** Prevents deployment of biased models that harm vulnerable populations  
**Regulatory:** Supports FDA algorithmic fairness requirements (2021 guidance)  
**Equity:** Centers Indigenous health, addresses systematic healthcare disparities  

**Market Opportunity:**
- 6,000 US hospitals Ã— $50K/year = $300M TAM
- 2,000 clinical trials/year Ã— $200K = $400M
- Total addressable market: $700M annually

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

- American Heart Association (cholesterol guidelines)
- CDC Health Disparities Reports (population health data)
- FDA AI/ML Medical Device Guidance (regulatory requirements)
- Northeastern University (hackathon support)

---

## ğŸ“ Contact

**GitHub:** [github.com/Varsh1009/meddata-guardian](https://github.com/Varsh1009/meddata-guardian)  
**Event:** Convergence 2026 Hackathon  
**Date:** January 30, 2026  

---

**Built with â¤ï¸ for healthcare equity and responsible AI**
